{"cells":[{"cell_type":"code","execution_count":81,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1656070608623,"user":{"displayName":"daren stoev","userId":"16546818881509107436"},"user_tz":-180},"id":"aF92NgedTlFQ","outputId":"c512a679-33e0-4aac-cda6-3811b2ee693e"},"outputs":[{"name":"stderr","output_type":"stream","text":["UsageError: Line magic function `%tensorflow_version` not found.\n"]}],"source":["%tensorflow_version 1.x"]},{"cell_type":"code","execution_count":82,"metadata":{"executionInfo":{"elapsed":3579,"status":"ok","timestamp":1656076386655,"user":{"displayName":"daren stoev","userId":"16546818881509107436"},"user_tz":-180},"id":"SQWOM7jXgHYI"},"outputs":[],"source":["\n","import tensorflow as tf\n","from datetime import datetime\n","import os"]},{"cell_type":"markdown","metadata":{"id":"JvOEZtX_inZ1"},"source":["AttentionWeighted Average\n"]},{"cell_type":"markdown","metadata":{"id":"vMWxmSR6j3W5"},"source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","# GitHub Code\n"]},{"cell_type":"code","execution_count":83,"metadata":{"cellView":"code","executionInfo":{"elapsed":283,"status":"ok","timestamp":1656076428537,"user":{"displayName":"daren stoev","userId":"16546818881509107436"},"user_tz":-180},"id":"ETfsaeBkiltN"},"outputs":[],"source":["#@title Default title text\n","from tensorflow.keras.layers import Layer, InputSpec\n","from tensorflow.keras import backend as K\n","from tensorflow.keras import initializers\n","\n","\n","class AttentionWeightedAverage(Layer):\n","    \"\"\"\n","    Computes a weighted average of the different channels across timesteps.\n","    Uses 1 parameter pr. channel to compute the attention value for\n","    a single timestep.\n","    \"\"\"\n","\n","    def __init__(self, return_attention=False, **kwargs):\n","        self.init = initializers.get('uniform')\n","        self.supports_masking = True\n","        self.return_attention = return_attention\n","        super(AttentionWeightedAverage, self).__init__(** kwargs)\n","\n","    def build(self, input_shape):\n","        self.input_spec = [InputSpec(ndim=3)]\n","        assert len(input_shape) == 3\n","\n","        self.W = self.add_weight(shape=(input_shape[2], 1),\n","                                 name='{}_W'.format(self.name),\n","                                 trainable=True,\n","                                 initializer=self.init)\n","        super(AttentionWeightedAverage, self).build(input_shape)\n","\n","    def call(self, x, mask=None):\n","        # computes a probability distribution over the timesteps\n","        # uses 'max trick' for numerical stability\n","        # reshape is done to avoid issue with Tensorflow\n","        # and 1-dimensional weights\n","        logits = K.dot(x, self.W)\n","        x_shape = K.shape(x)\n","        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n","        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n","\n","        # masked timesteps have zero weight\n","        if mask is not None:\n","            mask = K.cast(mask, K.floatx())\n","            ai = ai * mask\n","        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n","        weighted_input = x * K.expand_dims(att_weights)\n","        result = K.sum(weighted_input, axis=1)\n","        if self.return_attention:\n","            return [result, att_weights]\n","        return result\n","\n","    def get_output_shape_for(self, input_shape):\n","        return self.compute_output_shape(input_shape)\n","\n","    def compute_output_shape(self, input_shape):\n","        output_len = input_shape[2]\n","        if self.return_attention:\n","            return [(input_shape[0], output_len), (input_shape[0],\n","                                                   input_shape[1])]\n","        return (input_shape[0], output_len)\n","\n","    def compute_mask(self, input, input_mask=None):\n","        if isinstance(input_mask, list):\n","            return [None] * len(input_mask)\n","        else:\n","            return None"]},{"cell_type":"markdown","metadata":{"id":"-2BHRyOVjA-T"},"source":["Model"]},{"cell_type":"code","execution_count":84,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1656076428538,"user":{"displayName":"daren stoev","userId":"16546818881509107436"},"user_tz":-180},"id":"7jEhlSokizBN","outputId":"2e1466fe-fd3d-409a-c487-449b1faae537"},"outputs":[],"source":["from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.layers import Input, Embedding, Dense, LSTM, Bidirectional\n","from tensorflow.keras.layers import concatenate, Reshape, SpatialDropout1D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import backend as K\n","from tensorflow import config as config\n","#from .AttentionWeightedAverage import AttentionWeightedAverage\n","\n","\n","def textgenrnn_model(num_classes, cfg, context_size=None,\n","                     weights_path=None,\n","                     dropout=0.0,\n","                     optimizer=Adam(lr=4e-3)):\n","    '''\n","    Builds the model architecture for textgenrnn and\n","    loads the specified weights for the model.\n","    '''\n","\n","    input = Input(shape=(cfg['max_length'],), name='input')\n","    embedded = Embedding(num_classes, cfg['dim_embeddings'],\n","                         input_length=cfg['max_length'],\n","                         name='embedding')(input)\n","\n","    if dropout > 0.0:\n","        embedded = SpatialDropout1D(dropout, name='dropout')(embedded)\n","\n","    rnn_layer_list = []\n","    for i in range(cfg['rnn_layers']):\n","        prev_layer = embedded if i == 0 else rnn_layer_list[-1]\n","        rnn_layer_list.append(new_rnn(cfg, i+1)(prev_layer))\n","\n","    seq_concat = concatenate([embedded] + rnn_layer_list, name='rnn_concat')\n","    attention = AttentionWeightedAverage(name='attention')(seq_concat)\n","    output = Dense(num_classes, name='output', activation='softmax')(attention)\n","\n","    if context_size is None:\n","        model = Model(inputs=[input], outputs=[output])\n","        if weights_path is not None:\n","            model.load_weights(weights_path, by_name=True)\n","        model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n","\n","    else:\n","        context_input = Input(\n","            shape=(context_size,), name='context_input')\n","        context_reshape = Reshape((context_size,),\n","                                  name='context_reshape')(context_input)\n","        merged = concatenate([attention, context_reshape], name='concat')\n","        main_output = Dense(num_classes, name='context_output',\n","                            activation='softmax')(merged)\n","\n","        model = Model(inputs=[input, context_input],\n","                      outputs=[main_output, output])\n","        if weights_path is not None:\n","            model.load_weights(weights_path, by_name=True)\n","        model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n","                      loss_weights=[0.8, 0.2])\n","\n","    return model\n","\n","\n","'''\n","Create a new LSTM layer per parameters. Unfortunately,\n","each combination of parameters must be hardcoded.\n","The normal LSTMs use sigmoid recurrent activations\n","for parity with CuDNNLSTM:\n","https://github.com/keras-team/keras/issues/8860\n","'''\n","\n","'''\n","FIXME\n","From TensorFlow 2 you do not need to specify CuDNNLSTM.\n","You can just use LSTM with no activation function and it will\n","automatically use the CuDNN version.\n","This part can probably be cleaned up.\n","'''\n","\n","def new_rnn(cfg, layer_num):\n","    use_cudnnlstm = K.backend() == 'tensorflow' and len(config.get_visible_devices('GPU')) > 0\n","    if use_cudnnlstm:\n","        if cfg['rnn_bidirectional']:\n","            return Bidirectional(LSTM(cfg['rnn_size'],\n","                                           return_sequences=True),\n","                                 name='rnn_{}'.format(layer_num))\n","\n","        return LSTM(cfg['rnn_size'],\n","                         return_sequences=True,\n","                         name='rnn_{}'.format(layer_num))\n","    else:\n","        if cfg['rnn_bidirectional']:\n","            return Bidirectional(LSTM(cfg['rnn_size'],\n","                                      return_sequences=True,\n","                                      recurrent_activation='sigmoid'),\n","                                 name='rnn_{}'.format(layer_num))\n","\n","        return LSTM(cfg['rnn_size'],\n","                    return_sequences=True,\n","                    recurrent_activation='sigmoid',\n","                    name='rnn_{}'.format(layer_num))"]},{"cell_type":"markdown","metadata":{"id":"D4AALx9yjLXk"},"source":["Training"]},{"cell_type":"code","execution_count":85,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1656076428538,"user":{"displayName":"daren stoev","userId":"16546818881509107436"},"user_tz":-180},"id":"Nqq7KgWcjDt2"},"outputs":[],"source":["import numpy as np\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","#from .utils import textgenrnn_encode_cat\n","\n","\n","def generate_sequences_from_texts(texts, indices_list,\n","                                  textgenrnn, context_labels,\n","                                  batch_size=128):\n","    is_words = textgenrnn.config['word_level']\n","    is_single = textgenrnn.config['single_text']\n","    max_length = textgenrnn.config['max_length']\n","    meta_token = textgenrnn.META_TOKEN\n","\n","    if is_words:\n","        new_tokenizer = Tokenizer(filters='', char_level=True)\n","        new_tokenizer.word_index = textgenrnn.vocab\n","    else:\n","        new_tokenizer = textgenrnn.tokenizer\n","\n","    while True:\n","        np.random.shuffle(indices_list)\n","\n","        X_batch = []\n","        Y_batch = []\n","        context_batch = []\n","        count_batch = 0\n","\n","        for row in range(indices_list.shape[0]):\n","            text_index = indices_list[row, 0]\n","            end_index = indices_list[row, 1]\n","\n","            text = texts[text_index]\n","\n","            if not is_single:\n","                text = [meta_token] + list(text) + [meta_token]\n","\n","            if end_index > max_length:\n","                x = text[end_index - max_length: end_index + 1]\n","            else:\n","                x = text[0: end_index + 1]\n","            y = text[end_index + 1]\n","\n","            if y in textgenrnn.vocab:\n","                x = process_sequence([x], textgenrnn, new_tokenizer)\n","                y = textgenrnn_encode_cat([y], textgenrnn.vocab)\n","\n","                X_batch.append(x)\n","                Y_batch.append(y)\n","\n","                if context_labels is not None:\n","                    context_batch.append(context_labels[text_index])\n","\n","                count_batch += 1\n","\n","                if count_batch % batch_size == 0:\n","                    X_batch = np.squeeze(np.array(X_batch))\n","                    Y_batch = np.squeeze(np.array(Y_batch))\n","                    context_batch = np.squeeze(np.array(context_batch))\n","\n","                    # print(X_batch.shape)\n","\n","                    if context_labels is not None:\n","                        yield ([X_batch, context_batch], [Y_batch, Y_batch])\n","                    else:\n","                        yield (X_batch, Y_batch)\n","                    X_batch = []\n","                    Y_batch = []\n","                    context_batch = []\n","                    count_batch = 0\n","\n","\n","def process_sequence(X, textgenrnn, new_tokenizer):\n","    X = new_tokenizer.texts_to_sequences(X)\n","    X = sequence.pad_sequences(\n","        X, maxlen=textgenrnn.config['max_length'])\n","\n","    return X"]},{"cell_type":"markdown","metadata":{"id":"F9HQzON2jM1l"},"source":["Textgenrnn"]},{"cell_type":"code","execution_count":86,"metadata":{"executionInfo":{"elapsed":1351,"status":"ok","timestamp":1656076429887,"user":{"displayName":"daren stoev","userId":"16546818881509107436"},"user_tz":-180},"id":"dvcPeW68jPpi"},"outputs":[],"source":["import json\n","import re\n","\n","import numpy as np\n","import tensorflow as tf\n","import tqdm\n","from pkg_resources import resource_filename\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.preprocessing import LabelBinarizer\n","from tensorflow import config as config\n","from tensorflow.compat.v1.keras.backend import set_session\n","from tensorflow.keras.callbacks import LearningRateScheduler\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n","\n","#from .model import textgenrnn_model\n","#from .model_training import generate_sequences_from_texts\n","#from .utils import (\n"," #   generate_after_epoch,\n","  #  save_model_weights,\n","  #  textgenrnn_encode_sequence,\n","  #  textgenrnn_generate,\n","  #  textgenrnn_texts_from_file,\n","  #  textgenrnn_texts_from_file_context,\n","#)\n","\n","\n","class textgenrnn:\n","    META_TOKEN = '<s>'\n","    config = {\n","        'rnn_layers': 2,\n","        'rnn_size': 128,\n","        'rnn_bidirectional': False,\n","        'max_length': 40,\n","        'max_words': 10000,\n","        'dim_embeddings': 100,\n","        'word_level': False,\n","        'single_text': False\n","    }\n","    default_config = config.copy()\n","\n","    def __init__(self, weights_path=None,\n","                 vocab_path=None,\n","                 config_path=None,\n","                 name=\"textgenrnn\",\n","                 allow_growth=None):\n","\n","        if weights_path is None:\n","            weights_path = resource_filename(__name__,\n","                                             'textgenrnn_weights.hdf5')\n","\n","        if vocab_path is None:\n","            vocab_path = resource_filename(__name__,\n","                                           'textgenrnn_vocab.json')\n","\n","        if allow_growth is not None:\n","            c = tf.compat.v1.ConfigProto()\n","            c.gpu_options.allow_growth = True\n","            set_session(tf.compat.v1.Session(config=c))\n","\n","        if config_path is not None:\n","            with open(config_path, 'r',\n","                      encoding='utf8', errors='ignore') as json_file:\n","                self.config = json.load(json_file)\n","\n","        self.config.update({'name': name})\n","        self.default_config.update({'name': name})\n","\n","        with open(vocab_path, 'r',\n","                  encoding='utf8', errors='ignore') as json_file:\n","            self.vocab = json.load(json_file)\n","\n","        self.tokenizer = Tokenizer(filters='', lower=False, char_level=True)\n","        self.tokenizer.word_index = self.vocab\n","        self.num_classes = len(self.vocab) + 1\n","        self.model = textgenrnn_model(self.num_classes,\n","                                      cfg=self.config,\n","                                      weights_path=weights_path)\n","        self.indices_char = dict((self.vocab[c], c) for c in self.vocab)\n","\n","    def generate(self, n=1, return_as_list=False, prefix=None,\n","                 temperature=[1.0, 0.5, 0.2, 0.2],\n","                 max_gen_length=300, interactive=False,\n","                 top_n=3, progress=True):\n","        gen_texts = []\n","        iterable = tqdm.trange(n) if progress and n > 1 else range(n)\n","        for _ in iterable:\n","            gen_text, _ = textgenrnn_generate(self.model,\n","                                              self.vocab,\n","                                              self.indices_char,\n","                                              temperature,\n","                                              self.config['max_length'],\n","                                              self.META_TOKEN,\n","                                              self.config['word_level'],\n","                                              self.config.get(\n","                                                  'single_text', False),\n","                                              max_gen_length,\n","                                              interactive,\n","                                              top_n,\n","                                              prefix)\n","            if not return_as_list:\n","                print(\"{}\\n\".format(gen_text))\n","            gen_texts.append(gen_text)\n","        if return_as_list:\n","            return gen_texts\n","\n","    def generate_samples(self, n=3, temperatures=[0.2, 0.5, 1.0], **kwargs):\n","        for temperature in temperatures:\n","            print('#'*20 + '\\nTemperature: {}\\n'.format(temperature) +\n","                  '#'*20)\n","            self.generate(n, temperature=temperature, progress=False, **kwargs)\n","\n","    def train_on_texts(self, texts, context_labels=None,\n","                       batch_size=128,\n","                       num_epochs=50,\n","                       verbose=1,\n","                       new_model=False,\n","                       gen_epochs=1,\n","                       train_size=1.0,\n","                       max_gen_length=300,\n","                       validation=True,\n","                       dropout=0.0,\n","                       via_new_model=False,\n","                       save_epochs=0,\n","                       multi_gpu=False,\n","                       **kwargs):\n","\n","        if new_model and not via_new_model:\n","            self.train_new_model(texts,\n","                                 context_labels=context_labels,\n","                                 num_epochs=num_epochs,\n","                                 gen_epochs=gen_epochs,\n","                                 train_size=train_size,\n","                                 batch_size=batch_size,\n","                                 dropout=dropout,\n","                                 validation=validation,\n","                                 save_epochs=save_epochs,\n","                                 multi_gpu=multi_gpu,\n","                                 **kwargs)\n","            return\n","\n","        if context_labels:\n","            context_labels = LabelBinarizer().fit_transform(context_labels)\n","\n","        if self.config['word_level']:\n","            # If training word level, must add spaces around each\n","            # punctuation. https://stackoverflow.com/a/3645946/9314418\n","            punct = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\\\n\\\\t\\'‘’“”’–—…'\n","            for i in range(len(texts)):\n","                texts[i] = re.sub('([{}])'.format(punct), r' \\1 ', texts[i])\n","                texts[i] = re.sub(' {2,}', ' ', texts[i])\n","            texts = [text_to_word_sequence(text, filters='') for text in texts]\n","\n","        # calculate all combinations of text indices + token indices\n","        indices_list = [np.meshgrid(np.array(i), np.arange(\n","            len(text) + 1)) for i, text in enumerate(texts)]\n","        # indices_list = np.block(indices_list) # this hangs when indices_list is large enough\n","        # FIX BEGIN ------\n","        indices_list_o = np.block(indices_list[0])\n","        for i in range(len(indices_list)-1):\n","            tmp = np.block(indices_list[i+1])\n","            indices_list_o = np.concatenate([indices_list_o, tmp])\n","        indices_list = indices_list_o\n","        # FIX END ------\n","\n","        # If a single text, there will be 2 extra indices, so remove them\n","        # Also remove first sequences which use padding\n","        if self.config['single_text']:\n","            indices_list = indices_list[self.config['max_length']:-2, :]\n","\n","        indices_mask = np.random.rand(indices_list.shape[0]) < train_size\n","\n","        if multi_gpu:\n","            num_gpus = len(config.get_visible_devices('GPU'))\n","            batch_size = batch_size * num_gpus\n","\n","        gen_val = None\n","        val_steps = None\n","        if train_size < 1.0 and validation:\n","            indices_list_val = indices_list[~indices_mask, :]\n","            gen_val = generate_sequences_from_texts(\n","                texts, indices_list_val, self, context_labels, batch_size)\n","            val_steps = max(\n","                int(np.floor(indices_list_val.shape[0] / batch_size)), 1)\n","\n","        indices_list = indices_list[indices_mask, :]\n","\n","        num_tokens = indices_list.shape[0]\n","        assert num_tokens >= batch_size, \"Fewer tokens than batch_size.\"\n","\n","        level = 'word' if self.config['word_level'] else 'character'\n","        print(\"Training on {:,} {} sequences.\".format(num_tokens, level))\n","\n","        steps_per_epoch = max(int(np.floor(num_tokens / batch_size)), 1)\n","\n","        gen = generate_sequences_from_texts(\n","            texts, indices_list, self, context_labels, batch_size)\n","\n","        base_lr = 4e-3\n","\n","        # scheduler function must be defined inline.\n","        def lr_linear_decay(epoch):\n","            return (base_lr * (1 - (epoch / num_epochs)))\n","\n","        '''\n","        FIXME\n","        This part is a bit messy as we need to initialize the model within\n","        strategy.scope() when using multi-GPU. Can probably be cleaned up a bit.\n","        '''\n","\n","        if context_labels is not None:\n","            if new_model:\n","                weights_path = None\n","            else:\n","                weights_path = \"{}_weights.hdf5\".format(self.config['name'])\n","                self.save(weights_path)\n","\n","\n","            if multi_gpu:\n","                from tensorflow import distribute as distribute\n","                strategy = distribute.MirroredStrategy()\n","                with strategy.scope():\n","                    parallel_model = textgenrnn_model(self.num_classes,\n","                                                      dropout=dropout,\n","                                                      cfg=self.config,\n","                                                      context_size=context_labels.shape[1],\n","                                                      weights_path=weights_path)\n","                    parallel_model.compile(loss='categorical_crossentropy',\n","                                           optimizer=Adam(lr=4e-3))\n","                model_t = parallel_model\n","                print(\"Training on {} GPUs.\".format(num_gpus))\n","            else:\n","                model_t = self.model\n","        else:\n","            if multi_gpu:\n","                from tensorflow import distribute as distribute\n","                if new_model:\n","                    weights_path = None\n","                else:\n","                    weights_path = \"{}_weights.hdf5\".format(self.config['name'])\n","\n","                strategy = distribute.MirroredStrategy()\n","                with strategy.scope():\n","                # Do not locate model/merge on CPU since sample sizes are small.\n","                    parallel_model = textgenrnn_model(self.num_classes,\n","                                                      cfg=self.config,\n","                                                      weights_path=weights_path)\n","                    parallel_model.compile(loss='categorical_crossentropy',\n","                                           optimizer=Adam(lr=4e-3))\n","\n","                model_t = parallel_model\n","                print(\"Training on {} GPUs.\".format(num_gpus))\n","            else:\n","                model_t = self.model\n","\n","        model_t.fit(gen, steps_per_epoch=steps_per_epoch,\n","                              epochs=num_epochs,\n","                              callbacks=[\n","                                  LearningRateScheduler(\n","                                      lr_linear_decay),\n","                                  generate_after_epoch(\n","                                      self, gen_epochs,\n","                                      max_gen_length),\n","                                  save_model_weights(\n","                                      self, num_epochs,\n","                                      save_epochs)],\n","                              verbose=verbose,\n","                              max_queue_size=10,\n","                              validation_data=gen_val,\n","                              validation_steps=val_steps\n","                              )\n","\n","        # Keep the text-only version of the model if using context labels\n","        if context_labels is not None:\n","            self.model = Model(inputs=self.model.input[0],\n","                               outputs=self.model.output[1])\n","\n","    def train_new_model(self, texts, context_labels=None, num_epochs=50,\n","                        gen_epochs=1, batch_size=128, dropout=0.0,\n","                        train_size=1.0,\n","                        validation=True, save_epochs=0,\n","                        multi_gpu=False, **kwargs):\n","        self.config = self.default_config.copy()\n","        self.config.update(**kwargs)\n","\n","        print(\"Training new model w/ {}-layer, {}-cell {}LSTMs\".format(\n","            self.config['rnn_layers'], self.config['rnn_size'],\n","            'Bidirectional ' if self.config['rnn_bidirectional'] else ''\n","        ))\n","\n","        # Create text vocabulary for new texts\n","        # if word-level, lowercase; if char-level, uppercase\n","        self.tokenizer = Tokenizer(filters='',\n","                                   lower=self.config['word_level'],\n","                                   char_level=(not self.config['word_level']))\n","        self.tokenizer.fit_on_texts(texts)\n","\n","        # Limit vocab to max_words\n","        max_words = self.config['max_words']\n","        self.tokenizer.word_index = {k: v for (\n","            k, v) in self.tokenizer.word_index.items() if v <= max_words}\n","\n","        if not self.config.get('single_text', False):\n","            self.tokenizer.word_index[self.META_TOKEN] = len(\n","                self.tokenizer.word_index) + 1\n","        self.vocab = self.tokenizer.word_index\n","        self.num_classes = len(self.vocab) + 1\n","        self.indices_char = dict((self.vocab[c], c) for c in self.vocab)\n","\n","        # Create a new, blank model w/ given params\n","        self.model = textgenrnn_model(self.num_classes,\n","                                      dropout=dropout,\n","                                      cfg=self.config)\n","\n","        # Save the files needed to recreate the model\n","        with open('{}_vocab.json'.format(self.config['name']),\n","                  'w', encoding='utf8') as outfile:\n","            json.dump(self.tokenizer.word_index, outfile, ensure_ascii=False)\n","\n","        with open('{}_config.json'.format(self.config['name']),\n","                  'w', encoding='utf8') as outfile:\n","            json.dump(self.config, outfile, ensure_ascii=False)\n","\n","        self.train_on_texts(texts, new_model=True,\n","                            via_new_model=True,\n","                            context_labels=context_labels,\n","                            num_epochs=num_epochs,\n","                            gen_epochs=gen_epochs,\n","                            train_size=train_size,\n","                            batch_size=batch_size,\n","                            dropout=dropout,\n","                            validation=validation,\n","                            save_epochs=save_epochs,\n","                            multi_gpu=multi_gpu,\n","                            **kwargs)\n","\n","    def save(self, weights_path=\"textgenrnn_weights_saved.hdf5\"):\n","        self.model.save_weights(weights_path)\n","\n","    def load(self, weights_path):\n","        self.model = textgenrnn_model(self.num_classes,\n","                                      cfg=self.config,\n","                                      weights_path=weights_path)\n","\n","    def reset(self):\n","        self.config = self.default_config.copy()\n","        self.__init__(name=self.config['name'])\n","\n","    def train_from_file(self, file_path, header=True, delim=\"\\n\",\n","                        new_model=False, context=None,\n","                        is_csv=False, **kwargs):\n","\n","        context_labels = None\n","        if context:\n","            texts, context_labels = textgenrnn_texts_from_file_context(\n","                file_path)\n","        else:\n","            texts = textgenrnn_texts_from_file(file_path, header,\n","                                               delim, is_csv)\n","\n","        print(\"{:,} texts collected.\".format(len(texts)))\n","        if new_model:\n","            self.train_new_model(\n","                texts, context_labels=context_labels, **kwargs)\n","        else:\n","            self.train_on_texts(texts, context_labels=context_labels, **kwargs)\n","\n","    def train_from_largetext_file(self, file_path, new_model=True, **kwargs):\n","        with open(file_path, 'r', encoding='utf8', errors='ignore') as f:\n","            texts = [f.read()]\n","\n","        if new_model:\n","            self.train_new_model(\n","                texts, single_text=True, **kwargs)\n","        else:\n","            self.train_on_texts(texts, single_text=True, **kwargs)\n","\n","    def generate_to_file(self, destination_path, **kwargs):\n","        texts = self.generate(return_as_list=True, **kwargs)\n","        with open(destination_path, 'w', encoding=\"utf-8\") as f:\n","            for text in texts:\n","                f.write(\"{}\\n\".format(text))\n","\n","    def encode_text_vectors(self, texts, pca_dims=50, tsne_dims=None,\n","                            tsne_seed=None, return_pca=False,\n","                            return_tsne=False):\n","\n","        # if a single text, force it into a list:\n","        if isinstance(texts, str):\n","            texts = [texts]\n","\n","        vector_output = Model(inputs=self.model.input,\n","                              outputs=self.model.get_layer('attention').output)\n","        encoded_vectors = []\n","        maxlen = self.config['max_length']\n","        for text in texts:\n","            if self.config['word_level']:\n","                text = text_to_word_sequence(text, filters='')\n","            text_aug = [self.META_TOKEN] + list(text[0:maxlen])\n","            encoded_text = textgenrnn_encode_sequence(text_aug, self.vocab,\n","                                                      maxlen)\n","            encoded_vector = vector_output.predict(encoded_text)\n","            encoded_vectors.append(encoded_vector)\n","\n","        encoded_vectors = np.squeeze(np.array(encoded_vectors), axis=1)\n","        if pca_dims is not None:\n","            assert len(texts) > 1, \"Must use more than 1 text for PCA\"\n","            pca = PCA(pca_dims)\n","            encoded_vectors = pca.fit_transform(encoded_vectors)\n","\n","        if tsne_dims is not None:\n","            tsne = TSNE(tsne_dims, random_state=tsne_seed)\n","            encoded_vectors = tsne.fit_transform(encoded_vectors)\n","\n","        return_objects = encoded_vectors\n","        if return_pca or return_tsne:\n","            return_objects = [return_objects]\n","        if return_pca:\n","            return_objects.append(pca)\n","        if return_tsne:\n","            return_objects.append(tsne)\n","\n","        return return_objects\n","\n","    def similarity(self, text, texts, use_pca=True):\n","        text_encoded = self.encode_text_vectors(text, pca_dims=None)\n","        if use_pca:\n","            texts_encoded, pca = self.encode_text_vectors(texts,\n","                                                          return_pca=True)\n","            text_encoded = pca.transform(text_encoded)\n","        else:\n","            texts_encoded = self.encode_text_vectors(texts, pca_dims=None)\n","\n","        cos_similairity = cosine_similarity(text_encoded, texts_encoded)[0]\n","        text_sim_pairs = list(zip(texts, cos_similairity))\n","        text_sim_pairs = sorted(text_sim_pairs, key=lambda x: -x[1])\n","        return text_sim_pairs"]},{"cell_type":"markdown","metadata":{"id":"1pTThW5VjdV9"},"source":["Utils\n"]},{"cell_type":"code","execution_count":87,"metadata":{"executionInfo":{"elapsed":373,"status":"ok","timestamp":1656076430259,"user":{"displayName":"daren stoev","userId":"16546818881509107436"},"user_tz":-180},"id":"qzuYBIGWjeVf"},"outputs":[],"source":["import csv\n","import re\n","from random import shuffle\n","\n","import numpy as np\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.callbacks import Callback\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing import sequence\n","from tqdm import trange\n","\n","\n","def textgenrnn_sample(preds, temperature, interactive=False, top_n=3):\n","    '''\n","    Samples predicted probabilities of the next character to allow\n","    for the network to show \"creativity.\"\n","    '''\n","\n","    preds = np.asarray(preds).astype('float64')\n","\n","    if temperature is None or temperature == 0.0:\n","        return np.argmax(preds)\n","\n","    preds = np.log(preds + K.epsilon()) / temperature\n","    exp_preds = np.exp(preds)\n","    preds = exp_preds / np.sum(exp_preds)\n","    probas = np.random.multinomial(1, preds, 1)\n","\n","    if not interactive:\n","        index = np.argmax(probas)\n","\n","        # prevent function from being able to choose 0 (placeholder)\n","        # choose 2nd best index from preds\n","        if index == 0:\n","            index = np.argsort(preds)[-2]\n","    else:\n","        # return list of top N chars/words\n","        # descending order, based on probability\n","        index = (-preds).argsort()[:top_n]\n","\n","    return index\n","\n","\n","def textgenrnn_generate(model, vocab,\n","                        indices_char, temperature=0.5,\n","                        maxlen=40, meta_token='<s>',\n","                        word_level=False,\n","                        single_text=False,\n","                        max_gen_length=300,\n","                        interactive=False,\n","                        top_n=3,\n","                        prefix=None,\n","                        synthesize=False,\n","                        stop_tokens=[' ', '\\n']):\n","    '''\n","    Generates and returns a single text.\n","    '''\n","\n","    collapse_char = ' ' if word_level else ''\n","    end = False\n","\n","    # If generating word level, must add spaces around each punctuation.\n","    # https://stackoverflow.com/a/3645946/9314418\n","    if word_level and prefix:\n","        punct = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\\\n\\\\t\\'‘’“”’–—'\n","        prefix = re.sub('([{}])'.format(punct), r' \\1 ', prefix)\n","        prefix = re.sub(' {2,}', r' ', prefix)\n","        prefix_t = [x.lower() for x in prefix.split(' ')]\n","\n","    if not word_level and prefix:\n","        prefix_t = list(prefix)\n","\n","    if single_text:\n","        text = prefix_t if prefix else ['']\n","        max_gen_length += maxlen\n","    else:\n","        text = [meta_token] + prefix_t if prefix else [meta_token]\n","\n","    if not isinstance(temperature, list):\n","        temperature = [temperature]\n","\n","    if len(model.inputs) > 1:\n","        model = Model(inputs=model.inputs[0], outputs=model.outputs[1])\n","\n","    while not end and len(text) < max_gen_length:\n","        encoded_text = textgenrnn_encode_sequence(text[-maxlen:],\n","                                                  vocab, maxlen)\n","        next_temperature = temperature[(len(text) - 1) % len(temperature)]\n","\n","        if not interactive:\n","            # auto-generate text without user intervention\n","            next_index = textgenrnn_sample(\n","                model.predict(encoded_text, batch_size=1)[0],\n","                next_temperature)\n","            next_char = indices_char[next_index]\n","            text += [next_char]\n","            if next_char == meta_token or len(text) >= max_gen_length:\n","                end = True\n","            gen_break = (next_char in stop_tokens or word_level or\n","                         len(stop_tokens) == 0)\n","            if synthesize and gen_break:\n","                break\n","        else:\n","            # ask user what the next char/word should be\n","            options_index = textgenrnn_sample(\n","                model.predict(encoded_text, batch_size=1)[0],\n","                next_temperature,\n","                interactive=interactive,\n","                top_n=top_n\n","            )\n","            options = [indices_char[idx] for idx in options_index]\n","            print('Controls:\\n\\ts: stop.\\tx: backspace.\\to: write your own.')\n","            print('\\nOptions:')\n","\n","            for i, option in enumerate(options, 1):\n","                print('\\t{}: {}'.format(i, option))\n","\n","            print('\\nProgress: {}'.format(collapse_char.join(text)[3:]))\n","            print('\\nYour choice?')\n","            user_input = input('> ')\n","\n","            try:\n","                user_input = int(user_input)\n","                next_char = options[user_input-1]\n","                text += [next_char]\n","            except ValueError:\n","                if user_input == 's':\n","                    next_char = '<s>'\n","                    text += [next_char]\n","                elif user_input == 'o':\n","                    other = input('> ')\n","                    text += [other]\n","                elif user_input == 'x':\n","                    try:\n","                        del text[-1]\n","                    except IndexError:\n","                        pass\n","                else:\n","                    print('That\\'s not an option!')\n","\n","    # if single text, ignore sequences generated w/ padding\n","    # if not single text, remove the <s> meta_tokens\n","    if single_text:\n","        text = text[maxlen:]\n","    else:\n","        text = text[1:]\n","        if meta_token in text:\n","            text.remove(meta_token)\n","\n","    text_joined = collapse_char.join(text)\n","\n","    # If word level, remove spaces around punctuation for cleanliness.\n","    if word_level:\n","        left_punct = \"!%),.:;?@\\]_}\\\\n\\\\t'\"\n","        right_punct = \"$(\\[_\\\\n\\\\t'\"\n","        punct = '\\\\n\\\\t'\n","\n","        text_joined = re.sub(\" ([{}]) \".format(\n","            punct), r'\\1', text_joined)\n","        text_joined = re.sub(\" ([{}])\".format(\n","            left_punct), r'\\1', text_joined)\n","        text_joined = re.sub(\"([{}]) \".format(\n","            right_punct), r'\\1', text_joined)\n","        text_joined = re.sub('\" (.+?) \"', \n","            r'\"\\1\"', text_joined)\n","\n","    return text_joined, end\n","\n","\n","def textgenrnn_encode_sequence(text, vocab, maxlen):\n","    '''\n","    Encodes a text into the corresponding encoding for prediction with\n","    the model.\n","    '''\n","    encoded = np.array([vocab.get(x, 0) for x in text])\n","    return sequence.pad_sequences([encoded], maxlen=maxlen)\n","\n","\n","def textgenrnn_texts_from_file(file_path, header=True,\n","                               delim='\\n', is_csv=False):\n","    '''\n","    Retrieves texts from a newline-delimited file and returns as a list.\n","    '''\n","#trying to remove \\n\n","    with open(file_path, 'r', encoding='utf8', errors='ignore') as f:\n","        if header:\n","            f.readline()\n","        if is_csv:\n","            texts = []\n","            reader = csv.reader(f)\n","            for row in reader:\n","                if row:\n","                    texts.append(row[0])\n","        else:\n","            text_data = f.read()\n","            texts = text_data.split(delim)\n","\n","    return texts\n","\n","\n","def textgenrnn_texts_from_file_context(file_path, header=True):\n","    '''\n","    Retrieves texts+context from a two-column CSV.\n","    '''\n","\n","    with open(file_path, 'r', encoding='utf8', errors='ignore') as f:\n","        if header:\n","            f.readline()\n","        texts = []\n","        context_labels = []\n","        reader = csv.reader(f)\n","        for row in reader:\n","            if row:\n","                texts.append(row[0])\n","                context_labels.append(row[1])\n","\n","    return (texts, context_labels)\n","\n","\n","def textgenrnn_encode_cat(chars, vocab):\n","    '''\n","    One-hot encodes values at given chars efficiently by preallocating\n","    a zeros matrix.\n","    '''\n","\n","    a = np.float32(np.zeros((len(chars), len(vocab) + 1)))\n","    rows, cols = zip(*[(i, vocab.get(char, 0))\n","                       for i, char in enumerate(chars)])\n","    a[rows, cols] = 1\n","    return a\n","\n","\n","def synthesize(textgens, n=1, return_as_list=False, prefix='',\n","               temperature=[0.5, 0.2, 0.2], max_gen_length=300,\n","               progress=True, stop_tokens=[' ', '\\n']):\n","    \"\"\"Synthesizes texts using an ensemble of input models.\n","    \"\"\"\n","\n","    gen_texts = []\n","    iterable = trange(n) if progress and n > 1 else range(n)\n","    for _ in iterable:\n","        shuffle(textgens)\n","        gen_text = prefix\n","        end = False\n","        textgen_i = 0\n","        while not end:\n","            textgen = textgens[textgen_i % len(textgens)]\n","            gen_text, end = textgenrnn_generate(textgen.model,\n","                                                textgen.vocab,\n","                                                textgen.indices_char,\n","                                                temperature,\n","                                                textgen.config['max_length'],\n","                                                textgen.META_TOKEN,\n","                                                textgen.config['word_level'],\n","                                                textgen.config.get(\n","                                                    'single_text', False),\n","                                                max_gen_length,\n","                                                prefix=gen_text,\n","                                                synthesize=True,\n","                                                stop_tokens=stop_tokens)\n","            textgen_i += 1\n","        if not return_as_list:\n","            print(\"{}\\n\".format(gen_text))\n","        gen_texts.append(gen_text)\n","    if return_as_list:\n","        return gen_texts\n","\n","\n","def synthesize_to_file(textgens, destination_path, **kwargs):\n","    texts = synthesize(textgens, return_as_list=True, **kwargs)\n","    with open(destination_path, 'w') as f:\n","        for text in texts:\n","            f.write(\"{}\\n\".format(text))\n","\n","\n","class generate_after_epoch(Callback):\n","    def __init__(self, textgenrnn, gen_epochs, max_gen_length):\n","        super().__init__()\n","        self.textgenrnn = textgenrnn\n","        self.gen_epochs = gen_epochs\n","        self.max_gen_length = max_gen_length\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        if self.gen_epochs > 0 and (epoch+1) % self.gen_epochs == 0:\n","            self.textgenrnn.generate_samples(\n","                max_gen_length=self.max_gen_length)\n","\n","\n","class save_model_weights(Callback):\n","    def __init__(self, textgenrnn, num_epochs, save_epochs):\n","        super().__init__()\n","        self.textgenrnn = textgenrnn\n","        self.weights_name = textgenrnn.config['name']\n","        self.num_epochs = num_epochs\n","        self.save_epochs = save_epochs\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        if len(self.textgenrnn.model.inputs) > 1:\n","            self.textgenrnn.model = Model(inputs=self.model.input[0],\n","                                          outputs=self.model.output[1])\n","        if self.save_epochs > 0 and (epoch+1) % self.save_epochs == 0 and self.num_epochs != (epoch+1):\n","            print(\"Saving Model Weights — Epoch #{}\".format(epoch+1))\n","            self.textgenrnn.model.save_weights(\n","                \"{}_weights_epoch_{}.hdf5\".format(self.weights_name, epoch+1))\n","        else:\n","            self.textgenrnn.model.save_weights(\n","                \"{}_weights.hdf5\".format(self.weights_name))"]},{"cell_type":"markdown","metadata":{"id":"8QkO-Duukaqi"},"source":["# My part\n"]},{"cell_type":"markdown","metadata":{"id":"DFtlTP9xk2AD"},"source":["Initializing the models and the training"]},{"cell_type":"code","execution_count":116,"metadata":{"executionInfo":{"elapsed":266,"status":"ok","timestamp":1656076829339,"user":{"displayName":"daren stoev","userId":"16546818881509107436"},"user_tz":-180},"id":"yPCnIBZ-kgQn"},"outputs":[],"source":["model_cfg = {\n","    'word_level': True,   # set to True if want to train a word-level model (requires more data and smaller max_length)\n","    'rnn_size': 128,   # number of LSTM cells of each layer (128/256 recommended)\n","    'rnn_layers': 16,   # number of LSTM layers (>=2 recommended)\n","    'rnn_bidirectional': False,   # consider text both forwards and backward, can give a training boost\n","    'max_length': 5,   # number of tokens to consider before predicting the next (20-40 for characters, 5-10 for words recommended)\n","    'max_words': 25600,   # maximum number of words to model; the rest will be ignored (word-level model only)\n","}\n","\n","train_cfg = {\n","    'line_delimited': True,   # set to True if each text has its own line in the source file\n","    'num_epochs': 512,   # set higher to train the model for longer\n","    'gen_epochs': 2510,   # generates sample text from model after given number of epochs\n","    'train_size': 0.9,   # proportion of input data to train on: setting < 1.0 limits model from learning perfectly\n","    'dropout': 0.0,   # ignore a random proportion of source tokens each epoch, allowing model to generalize better\n","    'validation': False,   # If train__size < 1.0, test on holdout dataset; will make overall training slower\n","    'is_csv': False   # set to True if file is a CSV exported from Excel/BigQuery/pandas\n","}"]},{"cell_type":"markdown","metadata":{"id":"9NJP7rmqk5cO"},"source":["Uploading the file"]},{"cell_type":"code","execution_count":117,"metadata":{"executionInfo":{"elapsed":392,"status":"ok","timestamp":1656076831824,"user":{"displayName":"daren stoev","userId":"16546818881509107436"},"user_tz":-180},"id":"JPu2LV12k1BN"},"outputs":[],"source":["file_name = \"C:/Users/User/Desktop/auto_.txt\"\n","model_name = 'Linkin_Park_Lyrics'   # change to set file name of resulting trained models/texts"]},{"cell_type":"markdown","metadata":{"id":"M_tB4rsZlYNR"},"source":["Training the model\n"]},{"cell_type":"code","execution_count":118,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":432},"executionInfo":{"elapsed":41201,"status":"error","timestamp":1656076875056,"user":{"displayName":"daren stoev","userId":"16546818881509107436"},"user_tz":-180},"id":"oUxf3fGelaEe","outputId":"280d0bbb-3021-4094-9af7-2459a8344bb5"},"outputs":[{"name":"stdout","output_type":"stream","text":["1,093 texts collected.\n","Training new model w/ 16-layer, 128-cell LSTMs\n","Training on 7,720 word sequences.\n","Epoch 1/512\n","7/7 [==============================] - 77s 94ms/step - loss: 11.6102\n","Epoch 2/512\n","7/7 [==============================] - 1s 90ms/step - loss: 5.9643\n","Epoch 3/512\n","7/7 [==============================] - 1s 92ms/step - loss: 6.4144\n","Epoch 4/512\n","7/7 [==============================] - 1s 91ms/step - loss: 7.1904\n","Epoch 5/512\n","7/7 [==============================] - 1s 90ms/step - loss: 6.4311\n","Epoch 6/512\n","7/7 [==============================] - 1s 89ms/step - loss: 5.5628\n","Epoch 7/512\n","7/7 [==============================] - 1s 90ms/step - loss: 5.3554\n","Epoch 8/512\n","7/7 [==============================] - 1s 91ms/step - loss: 4.9002\n","Epoch 9/512\n","7/7 [==============================] - 1s 90ms/step - loss: 4.7608\n","Epoch 10/512\n","7/7 [==============================] - 1s 89ms/step - loss: 4.4859\n","Epoch 11/512\n","7/7 [==============================] - 1s 89ms/step - loss: 4.3014\n","Epoch 12/512\n","7/7 [==============================] - 1s 90ms/step - loss: 4.1931\n","Epoch 13/512\n","7/7 [==============================] - 1s 92ms/step - loss: 4.0682\n","Epoch 14/512\n","7/7 [==============================] - 1s 90ms/step - loss: 3.9791\n","Epoch 15/512\n","7/7 [==============================] - 1s 89ms/step - loss: 3.8965\n","Epoch 16/512\n","7/7 [==============================] - 1s 91ms/step - loss: 3.8450\n","Epoch 17/512\n","7/7 [==============================] - 1s 91ms/step - loss: 3.7373\n","Epoch 18/512\n","7/7 [==============================] - 1s 89ms/step - loss: 3.6597\n","Epoch 19/512\n","7/7 [==============================] - 1s 89ms/step - loss: 3.5788\n","Epoch 20/512\n","7/7 [==============================] - 1s 91ms/step - loss: 3.4659\n","Epoch 21/512\n","7/7 [==============================] - 1s 91ms/step - loss: 3.3958\n","Epoch 22/512\n","7/7 [==============================] - 1s 89ms/step - loss: 3.3266\n","Epoch 23/512\n","7/7 [==============================] - 1s 90ms/step - loss: 3.2214\n","Epoch 24/512\n","7/7 [==============================] - 1s 91ms/step - loss: 3.1399\n","Epoch 25/512\n","7/7 [==============================] - 1s 91ms/step - loss: 3.0297\n","Epoch 26/512\n","7/7 [==============================] - 1s 92ms/step - loss: 2.9640: 0s - loss: 3\n","Epoch 27/512\n","7/7 [==============================] - 1s 91ms/step - loss: 2.8785\n","Epoch 28/512\n","7/7 [==============================] - 1s 90ms/step - loss: 2.7992\n","Epoch 29/512\n","7/7 [==============================] - 1s 90ms/step - loss: 2.7266\n","Epoch 30/512\n","7/7 [==============================] - 1s 90ms/step - loss: 2.6552\n","Epoch 31/512\n","7/7 [==============================] - 1s 91ms/step - loss: 2.5534\n","Epoch 32/512\n","7/7 [==============================] - 1s 91ms/step - loss: 2.4778\n","Epoch 33/512\n","7/7 [==============================] - 1s 92ms/step - loss: 2.4392\n","Epoch 34/512\n","7/7 [==============================] - 1s 91ms/step - loss: 2.3404\n","Epoch 35/512\n","7/7 [==============================] - 1s 91ms/step - loss: 2.3067\n","Epoch 36/512\n","7/7 [==============================] - 1s 90ms/step - loss: 2.2499\n","Epoch 37/512\n","7/7 [==============================] - 1s 90ms/step - loss: 2.1760\n","Epoch 38/512\n","7/7 [==============================] - 1s 90ms/step - loss: 2.1191\n","Epoch 39/512\n","7/7 [==============================] - 1s 91ms/step - loss: 2.0811\n","Epoch 40/512\n","7/7 [==============================] - 1s 116ms/step - loss: 2.0372\n","Epoch 41/512\n","7/7 [==============================] - 1s 90ms/step - loss: 2.0064\n","Epoch 42/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.9571\n","Epoch 43/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.9293\n","Epoch 44/512\n","7/7 [==============================] - 1s 89ms/step - loss: 1.8947\n","Epoch 45/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.8450\n","Epoch 46/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.8146\n","Epoch 47/512\n","7/7 [==============================] - 1s 92ms/step - loss: 1.8388\n","Epoch 48/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.7875\n","Epoch 49/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.7327\n","Epoch 50/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.7611\n","Epoch 51/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.6669\n","Epoch 52/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.7055\n","Epoch 53/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.6554\n","Epoch 54/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.6249\n","Epoch 55/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.5822\n","Epoch 56/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.5845\n","Epoch 57/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.5562\n","Epoch 58/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.5291\n","Epoch 59/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.5207\n","Epoch 60/512\n","7/7 [==============================] - 1s 89ms/step - loss: 1.5053\n","Epoch 61/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.4781\n","Epoch 62/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.4592\n","Epoch 63/512\n","7/7 [==============================] - 1s 92ms/step - loss: 1.4464\n","Epoch 64/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.4384\n","Epoch 65/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.4272\n","Epoch 66/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.3962: 0s - loss: 1.396\n","Epoch 67/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.3920\n","Epoch 68/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.3784\n","Epoch 69/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.3643\n","Epoch 70/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.3361\n","Epoch 71/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.3546\n","Epoch 72/512\n","7/7 [==============================] - 1s 95ms/step - loss: 1.3074\n","Epoch 73/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.2977\n","Epoch 74/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.2979\n","Epoch 75/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.2730\n","Epoch 76/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.2860\n","Epoch 77/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.2759\n","Epoch 78/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.2541\n","Epoch 79/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.2456\n","Epoch 80/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.2167\n","Epoch 81/512\n","7/7 [==============================] - 1s 92ms/step - loss: 1.2204: 0s - loss: \n","Epoch 82/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.2232\n","Epoch 83/512\n","7/7 [==============================] - 1s 89ms/step - loss: 1.2118\n","Epoch 84/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.1907\n","Epoch 85/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.1971\n","Epoch 86/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.1810\n","Epoch 87/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.1859\n","Epoch 88/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.1586\n","Epoch 89/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.1440\n","Epoch 90/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.1776\n","Epoch 91/512\n","7/7 [==============================] - 1s 89ms/step - loss: 1.1491\n","Epoch 92/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.1540\n","Epoch 93/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.1281\n","Epoch 94/512\n","7/7 [==============================] - 1s 93ms/step - loss: 1.1246\n","Epoch 95/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.1308\n","Epoch 96/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.1173\n","Epoch 97/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.1169\n","Epoch 98/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.1067\n","Epoch 99/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.1163\n","Epoch 100/512\n","7/7 [==============================] - 1s 89ms/step - loss: 1.0903\n","Epoch 101/512\n","7/7 [==============================] - 1s 95ms/step - loss: 1.1267\n","Epoch 102/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.1189\n","Epoch 103/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.1082\n","Epoch 104/512\n","7/7 [==============================] - 1s 92ms/step - loss: 1.0870\n","Epoch 105/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.0852\n","Epoch 106/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.0866\n","Epoch 107/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.0772: 0s - loss: 1.\n","Epoch 108/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.0903\n","Epoch 109/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.0612\n","Epoch 110/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.0770: 0s - loss: 1.077\n","Epoch 111/512\n","7/7 [==============================] - 1s 92ms/step - loss: 1.0515\n","Epoch 112/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.0468\n","Epoch 113/512\n","7/7 [==============================] - 1s 92ms/step - loss: 1.0457\n","Epoch 114/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.0424\n","Epoch 115/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.0418\n","Epoch 116/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.0332\n","Epoch 117/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.0313\n","Epoch 118/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.0357\n","Epoch 119/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.0218\n","Epoch 120/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.0391\n","Epoch 121/512\n","7/7 [==============================] - 1s 92ms/step - loss: 1.0246\n","Epoch 122/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.0226\n","Epoch 123/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.0095\n","Epoch 124/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.0263\n","Epoch 125/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9986\n","Epoch 126/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.0164\n","Epoch 127/512\n","7/7 [==============================] - 1s 92ms/step - loss: 1.0067: 0s - loss: 1.0\n","Epoch 128/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.0521\n","Epoch 129/512\n","7/7 [==============================] - 1s 91ms/step - loss: 1.0286\n","Epoch 130/512\n","7/7 [==============================] - 1s 90ms/step - loss: 1.0062\n","Epoch 131/512\n","7/7 [==============================] - 1s 92ms/step - loss: 1.0223\n","Epoch 132/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9947\n","Epoch 133/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9836\n","Epoch 134/512\n","7/7 [==============================] - 1s 92ms/step - loss: 1.0082\n","Epoch 135/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9992\n","Epoch 136/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9887\n","Epoch 137/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9903\n","Epoch 138/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9922\n","Epoch 139/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9930\n","Epoch 140/512\n","7/7 [==============================] - 1s 94ms/step - loss: 0.9801\n","Epoch 141/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9564\n","Epoch 142/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9876\n","Epoch 143/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9743\n","Epoch 144/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9957\n","Epoch 145/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9816\n","Epoch 146/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9484\n","Epoch 147/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9789\n","Epoch 148/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9608\n","Epoch 149/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9816\n","Epoch 150/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9633\n","Epoch 151/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9506\n","Epoch 152/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9682\n","Epoch 153/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9601\n","Epoch 154/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9534\n","Epoch 155/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9519\n","Epoch 156/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9743\n","Epoch 157/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9453\n","Epoch 158/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9542\n","Epoch 159/512\n","7/7 [==============================] - 1s 98ms/step - loss: 0.9823\n","Epoch 160/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9375\n","Epoch 161/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9615: 0s - loss: 0.\n","Epoch 162/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9492\n","Epoch 163/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9481\n","Epoch 164/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9417\n","Epoch 165/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9576\n","Epoch 166/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9434\n","Epoch 167/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9612\n","Epoch 168/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9533\n","Epoch 169/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9479\n","Epoch 170/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9469\n","Epoch 171/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9412\n","Epoch 172/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9551\n","Epoch 173/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9385\n","Epoch 174/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9583\n","Epoch 175/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9305\n","Epoch 176/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9531: 0s - loss: 0.953\n","Epoch 177/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9153\n","Epoch 178/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9605\n","Epoch 179/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9348\n","Epoch 180/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9191\n","Epoch 181/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9306\n","Epoch 182/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9270\n","Epoch 183/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9405: 0s - loss: 0.92\n","Epoch 184/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9364\n","Epoch 185/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9310\n","Epoch 186/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9255: 0s - loss: 0.925\n","Epoch 187/512\n","7/7 [==============================] - 1s 93ms/step - loss: 0.9233\n","Epoch 188/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9264\n","Epoch 189/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9431\n","Epoch 190/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9437\n","Epoch 191/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9109\n","Epoch 192/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9261: 0s - loss: 0.93\n","Epoch 193/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9222\n","Epoch 194/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9394\n","Epoch 195/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9141\n","Epoch 196/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9241\n","Epoch 197/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9215\n","Epoch 198/512\n","7/7 [==============================] - 1s 89ms/step - loss: 0.9379\n","Epoch 199/512\n","7/7 [==============================] - 1s 93ms/step - loss: 0.9109\n","Epoch 200/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9282\n","Epoch 201/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9056: 0s - loss: 0.\n","Epoch 202/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9666\n","Epoch 203/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8990\n","Epoch 204/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9351\n","Epoch 205/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9295\n","Epoch 206/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9139\n","Epoch 207/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9205\n","Epoch 208/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9204\n","Epoch 209/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9391\n","Epoch 210/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9156\n","Epoch 211/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9184\n","Epoch 212/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9067\n","Epoch 213/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9204\n","Epoch 214/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9255\n","Epoch 215/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8965\n","Epoch 216/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9263\n","Epoch 217/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9176\n","Epoch 218/512\n","7/7 [==============================] - 1s 95ms/step - loss: 0.9283\n","Epoch 219/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9105\n","Epoch 220/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9147\n","Epoch 221/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9107\n","Epoch 222/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9101\n","Epoch 223/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9004\n","Epoch 224/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9073\n","Epoch 225/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9377\n","Epoch 226/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8868\n","Epoch 227/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9172\n","Epoch 228/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9168\n","Epoch 229/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9099: 0s - loss: 0\n","Epoch 230/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9107\n","Epoch 231/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9073\n","Epoch 232/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9142: 0s - loss: \n","Epoch 233/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9072\n","Epoch 234/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9094\n","Epoch 235/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9123\n","Epoch 236/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8820\n","Epoch 237/512\n","7/7 [==============================] - 1s 99ms/step - loss: 0.9191\n","Epoch 238/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9007\n","Epoch 239/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9158\n","Epoch 240/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9163\n","Epoch 241/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9005\n","Epoch 242/512\n","7/7 [==============================] - 1s 93ms/step - loss: 0.9122\n","Epoch 243/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9297\n","Epoch 244/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8962\n","Epoch 245/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.8941\n","Epoch 246/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9047\n","Epoch 247/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9080\n","Epoch 248/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8887\n","Epoch 249/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9209\n","Epoch 250/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9009\n","Epoch 251/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8910\n","Epoch 252/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9207\n","Epoch 253/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9063\n","Epoch 254/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9025\n","Epoch 255/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9029\n","Epoch 256/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.8908\n","Epoch 257/512\n","7/7 [==============================] - 1s 95ms/step - loss: 0.9119\n","Epoch 258/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9124\n","Epoch 259/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9053\n","Epoch 260/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9104\n","Epoch 261/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8709\n","Epoch 262/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9136\n","Epoch 263/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.8901\n","Epoch 264/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9114\n","Epoch 265/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8980\n","Epoch 266/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9099\n","Epoch 267/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8950\n","Epoch 268/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9105\n","Epoch 269/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8994\n","Epoch 270/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9059: 0s - loss: 0.9\n","Epoch 271/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9006\n","Epoch 272/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8932\n","Epoch 273/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9104\n","Epoch 274/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.8861\n","Epoch 275/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9345: 0s - loss: 0.93\n","Epoch 276/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.8960\n","Epoch 277/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9001\n","Epoch 278/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8980\n","Epoch 279/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8957\n","Epoch 280/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9103\n","Epoch 281/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9032\n","Epoch 282/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9050: 0s - loss: 0\n","Epoch 283/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9056\n","Epoch 284/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9107\n","Epoch 285/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8877\n","Epoch 286/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.8811\n","Epoch 287/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9198\n","Epoch 288/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8926\n","Epoch 289/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.8969\n","Epoch 290/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9061\n","Epoch 291/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.8982\n","Epoch 292/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9208\n","Epoch 293/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8779\n","Epoch 294/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8896: 0s - loss: \n","Epoch 295/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8905\n","Epoch 296/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9092\n","Epoch 297/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8878\n","Epoch 298/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9077\n","Epoch 299/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8918\n","Epoch 300/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8932\n","Epoch 301/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8876\n","Epoch 302/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8801\n","Epoch 303/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9223\n","Epoch 304/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9095\n","Epoch 305/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.8730\n","Epoch 306/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9024\n","Epoch 307/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8832\n","Epoch 308/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9025\n","Epoch 309/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.8788\n","Epoch 310/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9030\n","Epoch 311/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9146\n","Epoch 312/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.8798\n","Epoch 313/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8961\n","Epoch 314/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8889\n","Epoch 315/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8807\n","Epoch 316/512\n","7/7 [==============================] - 1s 100ms/step - loss: 0.9007\n","Epoch 317/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9009\n","Epoch 318/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8862\n","Epoch 319/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8915\n","Epoch 320/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8661\n","Epoch 321/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9101\n","Epoch 322/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8818\n","Epoch 323/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8956\n","Epoch 324/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8878\n","Epoch 325/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8927: 0s - loss: \n","Epoch 326/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8694\n","Epoch 327/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9013\n","Epoch 328/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8900\n","Epoch 329/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8902\n","Epoch 330/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8863\n","Epoch 331/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8972\n","Epoch 332/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.8872\n","Epoch 333/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8910\n","Epoch 334/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8848\n","Epoch 335/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9100\n","Epoch 336/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8861\n","Epoch 337/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8836\n","Epoch 338/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9082\n","Epoch 339/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8577\n","Epoch 340/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9150\n","Epoch 341/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8775\n","Epoch 342/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9023\n","Epoch 343/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8880\n","Epoch 344/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8901\n","Epoch 345/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9010: 0s - loss: \n","Epoch 346/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8901\n","Epoch 347/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.8838\n","Epoch 348/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8965\n","Epoch 349/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8747\n","Epoch 350/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8796\n","Epoch 351/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8963\n","Epoch 352/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8742\n","Epoch 353/512\n","7/7 [==============================] - 1s 97ms/step - loss: 0.9031\n","Epoch 354/512\n","7/7 [==============================] - 1s 93ms/step - loss: 0.8980\n","Epoch 355/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8903\n","Epoch 356/512\n","7/7 [==============================] - 1s 93ms/step - loss: 0.8969\n","Epoch 357/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.8878\n","Epoch 358/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8833: 0s - loss: 0\n","Epoch 359/512\n","7/7 [==============================] - 1s 94ms/step - loss: 0.9022\n","Epoch 360/512\n","7/7 [==============================] - 1s 95ms/step - loss: 0.8762\n","Epoch 361/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8779\n","Epoch 362/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8792\n","Epoch 363/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9045\n","Epoch 364/512\n","7/7 [==============================] - 1s 93ms/step - loss: 0.8865: 0s - loss: 0\n","Epoch 365/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8878\n","Epoch 366/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8900\n","Epoch 367/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8997\n","Epoch 368/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8718\n","Epoch 369/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8920\n","Epoch 370/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8997\n","Epoch 371/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8810\n","Epoch 372/512\n","7/7 [==============================] - 1s 97ms/step - loss: 0.8997\n","Epoch 373/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8853\n","Epoch 374/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9025\n","Epoch 375/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8781\n","Epoch 376/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.9024\n","Epoch 377/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8749\n","Epoch 378/512\n","7/7 [==============================] - 1s 93ms/step - loss: 0.8883\n","Epoch 379/512\n","7/7 [==============================] - 1s 93ms/step - loss: 0.8771\n","Epoch 380/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8844\n","Epoch 381/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8873\n","Epoch 382/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.8962\n","Epoch 383/512\n","7/7 [==============================] - 1s 93ms/step - loss: 0.8868\n","Epoch 384/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8797\n","Epoch 385/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8803\n","Epoch 386/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8866\n","Epoch 387/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8919\n","Epoch 388/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8566\n","Epoch 389/512\n","7/7 [==============================] - 1s 100ms/step - loss: 0.8912\n","Epoch 390/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8920\n","Epoch 391/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8729\n","Epoch 392/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8990\n","Epoch 393/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8856\n","Epoch 394/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.8801\n","Epoch 395/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8697\n","Epoch 396/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8910\n","Epoch 397/512\n","7/7 [==============================] - 1s 93ms/step - loss: 0.8930\n","Epoch 398/512\n","7/7 [==============================] - 1s 93ms/step - loss: 0.8739\n","Epoch 399/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8794\n","Epoch 400/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8969\n","Epoch 401/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8665\n","Epoch 402/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8927\n","Epoch 403/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8831\n","Epoch 404/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8770\n","Epoch 405/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9013\n","Epoch 406/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8700\n","Epoch 407/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8545\n","Epoch 408/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.8991\n","Epoch 409/512\n","7/7 [==============================] - 1s 93ms/step - loss: 0.8855\n","Epoch 410/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8752: 0s - loss: 0\n","Epoch 411/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9077\n","Epoch 412/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8672\n","Epoch 413/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8803: 0s - loss: \n","Epoch 414/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8809\n","Epoch 415/512\n","7/7 [==============================] - 1s 93ms/step - loss: 0.8806\n","Epoch 416/512\n","7/7 [==============================] - 1s 93ms/step - loss: 0.8856\n","Epoch 417/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8688\n","Epoch 418/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9000\n","Epoch 419/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8818\n","Epoch 420/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8808\n","Epoch 421/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8789\n","Epoch 422/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8656\n","Epoch 423/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.9050\n","Epoch 424/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8923\n","Epoch 425/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8701: 0s - loss: 0.85\n","Epoch 426/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8841\n","Epoch 427/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8890\n","Epoch 428/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8587\n","Epoch 429/512\n","7/7 [==============================] - 1s 93ms/step - loss: 0.9200\n","Epoch 430/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8563\n","Epoch 431/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8790\n","Epoch 432/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8819\n","Epoch 433/512\n","7/7 [==============================] - 1s 93ms/step - loss: 0.8853\n","Epoch 434/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8757\n","Epoch 435/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8871\n","Epoch 436/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8865\n","Epoch 437/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8565\n","Epoch 438/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8855\n","Epoch 439/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8800\n","Epoch 440/512\n","7/7 [==============================] - 1s 93ms/step - loss: 0.8496\n","Epoch 441/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8956\n","Epoch 442/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8979\n","Epoch 443/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8630\n","Epoch 444/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8753\n","Epoch 445/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8765\n","Epoch 446/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8749\n","Epoch 447/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.8828\n","Epoch 448/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8728\n","Epoch 449/512\n","7/7 [==============================] - 1s 94ms/step - loss: 0.8761\n","Epoch 450/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8858\n","Epoch 451/512\n","7/7 [==============================] - 1s 93ms/step - loss: 0.8866\n","Epoch 452/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8794\n","Epoch 453/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8640\n","Epoch 454/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8879\n","Epoch 455/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8940\n","Epoch 456/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8741\n","Epoch 457/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8803\n","Epoch 458/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8631\n","Epoch 459/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8862\n","Epoch 460/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8892\n","Epoch 461/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8903\n","Epoch 462/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8582\n","Epoch 463/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8688\n","Epoch 464/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8691\n","Epoch 465/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8888\n","Epoch 466/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8797\n","Epoch 467/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8688\n","Epoch 468/512\n","7/7 [==============================] - 1s 95ms/step - loss: 0.8731\n","Epoch 469/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8780\n","Epoch 470/512\n","7/7 [==============================] - 1s 93ms/step - loss: 0.8917\n","Epoch 471/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8476\n","Epoch 472/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8922\n","Epoch 473/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8650\n","Epoch 474/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8838\n","Epoch 475/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8707\n","Epoch 476/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8766\n","Epoch 477/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8666\n","Epoch 478/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8789\n","Epoch 479/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8861\n","Epoch 480/512\n","7/7 [==============================] - 1s 93ms/step - loss: 0.8745\n","Epoch 481/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8737\n","Epoch 482/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8722\n","Epoch 483/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8814\n","Epoch 484/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8784\n","Epoch 485/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8839\n","Epoch 486/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8779\n","Epoch 487/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8848\n","Epoch 488/512\n","7/7 [==============================] - 1s 102ms/step - loss: 0.8848\n","Epoch 489/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8511\n","Epoch 490/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8739\n","Epoch 491/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8754\n","Epoch 492/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8806\n","Epoch 493/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8710\n","Epoch 494/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8817\n","Epoch 495/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8869\n","Epoch 496/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8736\n","Epoch 497/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8757\n","Epoch 498/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8681\n","Epoch 499/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8699\n","Epoch 500/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8878\n","Epoch 501/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8742\n","Epoch 502/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8636\n","Epoch 503/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.8835\n","Epoch 504/512\n","7/7 [==============================] - 1s 90ms/step - loss: 0.8727\n","Epoch 505/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.9026\n","Epoch 506/512\n","7/7 [==============================] - 1s 93ms/step - loss: 0.8628\n","Epoch 507/512\n","7/7 [==============================] - ETA: 0s - loss: 0.874 - 1s 92ms/step - loss: 0.8746\n","Epoch 508/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8683\n","Epoch 509/512\n","7/7 [==============================] - 1s 92ms/step - loss: 0.8769\n","Epoch 510/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8775\n","Epoch 511/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8635\n","Epoch 512/512\n","7/7 [==============================] - 1s 91ms/step - loss: 0.8836\n"]}],"source":["textgen = textgenrnn(name=model_name)\n","\n","train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n","\n","train_function(\n","    file_path=file_name,\n","    new_model=True,\n","    num_epochs=train_cfg['num_epochs'],\n","    gen_epochs=train_cfg['gen_epochs'],\n","    batch_size=1024,\n","    train_size=train_cfg['train_size'],\n","    dropout=train_cfg['dropout'],\n","    validation=train_cfg['validation'],\n","    is_csv=train_cfg['is_csv'],\n","    rnn_layers=model_cfg['rnn_layers'],\n","    rnn_size=model_cfg['rnn_size'],\n","    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n","    max_length=model_cfg['max_length'],\n","    dim_embeddings=100,\n","    word_level=model_cfg['word_level'])"]},{"cell_type":"code","execution_count":119,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":302,"status":"error","timestamp":1656076762755,"user":{"displayName":"daren stoev","userId":"16546818881509107436"},"user_tz":-180},"id":"VbinX05knG2Q","outputId":"1a89a82b-b501-4b89-f865-8ada2cbda6a1"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:51<00:00,  1.92it/s]\n"]}],"source":["#this temperature schedule cycles between 1 very unexpected token, 1 unexpected token, 2 expected tokens, repeat.\n","# changing the temperature schedule can result in wildly different output!\n","temperature = [0.2, 0.5, 0.2, 0.2]   \n","prefix = None   # if you want each generated text to start with a given seed text\n","\n","if train_cfg['line_delimited']:\n","  n = 100\n","  max_gen_length = 60 if model_cfg['word_level'] else 300\n","else:\n","  n = 1\n","  max_gen_length = 256 if model_cfg['word_level'] else 10000\n","  \n","timestring = datetime.now().strftime('%Y%m%d_%H%M%S')\n","gen_file = '{}_gentext_{}.txt'.format(model_name, timestring)\n","\n","textgen.generate_to_file(gen_file,\n","                         temperature=temperature,\n","                         prefix=prefix,\n","                         n=n,\n","                         max_gen_length=max_gen_length)\n"]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyPZUBkBZn9GZjJ4vfnQWajW","name":"TextBot - textgnrnn","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.12 ('tf_gpu')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"23e7a0b0a1ade771721f510eeefb26c42b77578caec684a32e14f7e5228bf389"}}},"nbformat":4,"nbformat_minor":0}
